{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqRUMqU6g3ip"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain==0.1.0 openai==1.7.2 langchain-openai==0.0.2 langchain-community==0.0.12 langchainhub==0.1.14\n",
        "# !pip install chromadb==0.5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MCayCqvp08HN",
        "outputId": "bc0af5da-91d1-45b2-97eb-c1dfc153a2e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.1.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install langchain_openai\n",
        "# !pip install langchain\n",
        "# !pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKzUWAyx7Umq"
      },
      "source": [
        "https://python.langchain.com/v0.1/docs/use_cases/chatbots/#chat-retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5ohIYq_xLt"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIbwmQtjzhOj"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "OPEN_API_KEY = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ol9ys3z18h"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1p90CbD2bao",
        "outputId": "8a58650e-99a8-46ff-ee0c-d7952e2f7c22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Medicaid managed care is a system in which states contract with managed care organizations (MCOs) to provide healthcare services to Medicaid beneficiaries. These MCOs are responsible for coordinating and delivering healthcare services to enrollees in exchange for a fixed monthly payment per enrollee. Medicaid managed care aims to improve access to care, enhance quality, and control costs for Medicaid beneficiaries.')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, openai_api_key = OPEN_API_KEY)\n",
        "\n",
        "messages = [\n",
        "     SystemMessage(\n",
        "         content=\"\"\"You're an assistant knowledgeable about\n",
        "         healthcare. Only answer healthcare-related questions.\"\"\"\n",
        "     ),\n",
        "     HumanMessage(content=\"What is Medicaid managed care?\"),\n",
        "]\n",
        "chat_model.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0nNFNew23b2",
        "outputId": "57214054-d817-4aa8-a4f5-285ad8983e40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I'm unable to provide information on Chinese politics. If you have any other questions about politics in general, feel free to ask!\")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages3 = [\n",
        "     SystemMessage(\n",
        "         content=\"\"\"You're an assistant knowledgeable about\n",
        "         politics. DO NOT answer any questions about Chinese politics.\"\"\"\n",
        "     ),\n",
        "     HumanMessage(content=\"Tell me about Xi Jinping?\"),\n",
        "]\n",
        "chat_model.invoke(messages3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef9G3WXC3uJg",
        "outputId": "dc586e23-7330-45db-fbdd-2c21b40321ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Yes, Bob had a great stay at the hospital.')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_system_template_str = \"\"\"Your job is to use patient\n",
        "reviews to answer questions about their experience at a\n",
        "hospital. Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information\n",
        "that's not from the context. If you don't know an answer, say\n",
        "you don't know.\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=review_system_template_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"], template=\"{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "context = \"Bob had a great stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "review_prompt_template.format_messages(context=context, question=question)\n",
        "\n",
        "review_chain = review_prompt_template | chat_model\n",
        "review_chain.invoke({\"context\": context, \"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8VoVtuO_mEs"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osLy9KzyBziS"
      },
      "outputs": [],
      "source": [
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsKj6hw2_2BH"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "REVIEWS_CSV_PATH = \"./sample_data/reviews.csv\"\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
        "\n",
        "loader = CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\"review\")\n",
        "reviews = loader.load()\n",
        "\n",
        "reviews_vector_db = Chroma.from_documents(\n",
        "    reviews, OpenAIEmbeddings(openai_api_key = OPEN_API_KEY), persist_directory=REVIEWS_CHROMA_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmLkZWed-9aG",
        "outputId": "646bb60a-9cae-4b43-999f-8f62d1e732b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review_id: 73\n",
            "visit_id: 7696\n",
            "review: I had a frustrating experience at the hospital. The communication between the medical staff and me was unclear, leading to misunderstandings about my treatment plan. Improvement is needed in this area.\n",
            "physician_name: Maria Thompson\n",
            "hospital_name: Little-Spencer\n",
            "patient_name: Terri Smith\n",
            "review_id: 73\n",
            "visit_id: 7696\n",
            "review: I had a frustrating experience at the hospital. The communication between the medical staff and me was unclear, leading to misunderstandings about my treatment plan. Improvement is needed in this area.\n",
            "physician_name: Maria Thompson\n",
            "hospital_name: Little-Spencer\n",
            "patient_name: Terri Smith\n",
            "review_id: 73\n",
            "visit_id: 7696\n",
            "review: I had a frustrating experience at the hospital. The communication between the medical staff and me was unclear, leading to misunderstandings about my treatment plan. Improvement is needed in this area.\n",
            "physician_name: Maria Thompson\n",
            "hospital_name: Little-Spencer\n",
            "patient_name: Terri Smith\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
        "\n",
        "# ...\n",
        "\n",
        "reviews_retriever  = reviews_vector_db.as_retriever(k=10)\n",
        "\n",
        "question = \"\"\"Has anyone complained about\n",
        "           communication with the hospital staff?\"\"\"\n",
        "relevant_docs = reviews_vector_db.similarity_search(question, k=3)\n",
        "\n",
        "print(relevant_docs[0].page_content)\n",
        "print(relevant_docs[1].page_content)\n",
        "print(relevant_docs[2].page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt3hJZsyDzge"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
        "\n",
        "# ...\n",
        "\n",
        "reviews_vector_db = Chroma(\n",
        "    persist_directory=REVIEWS_CHROMA_PATH,\n",
        "    embedding_function=OpenAIEmbeddings(openai_api_key = OPEN_API_KEY)\n",
        ")\n",
        "\n",
        "reviews_retriever  = reviews_vector_db.as_retriever(k=10)\n",
        "\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    | review_prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_A9APKEI8ebs",
        "outputId": "5a4b8437-3d8f-4b35-9079-4361fdec09a4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Yes, a patient named Terri Smith complained about the communication between the medical staff and herself at the hospital. She mentioned that the communication was unclear, leading to misunderstandings about her treatment plan. Terri Smith suggested that improvement is needed in this area.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"Has anyone complained about\n",
        "           communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "13ZPT1EPrSfD",
        "outputId": "1a48ab0a-f7b3-4e40-b5c0-f6a017e69892"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your previous question was: \"What\\'s my previous question?\"'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question2 = \"\"\"what's my previous question?\"\"\"\n",
        "review_chain.invoke(question2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng_JfQmPrHnL",
        "outputId": "16171ffa-fe07-4374-cadb-f4cf21976c7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Has anyone complained about communication with the hospital staff?',\n",
              " 'chat_history': [HumanMessage(content='Has anyone complained about communication with the hospital staff?'),\n",
              "  AIMessage(content='Yes, Terri Smith has complained about the communication between the medical staff and herself at the hospital. She mentioned that the unclear communication led to misunderstandings about her treatment plan and expressed the need for improvement in this area.')],\n",
              " 'answer': 'Yes, Terri Smith has complained about the communication between the medical staff and herself at the hospital. She mentioned that the unclear communication led to misunderstandings about her treatment plan and expressed the need for improvement in this area.'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# prompt\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=review_system_template_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"], template=\"{question}\"\n",
        "    )\n",
        ")\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    chat_model,\n",
        "    reviews_retriever,\n",
        "    memory=memory,\n",
        "    verbose=False,\n",
        "    combine_docs_chain_kwargs={\"prompt\": review_prompt_template},\n",
        ")\n",
        "\n",
        "question = \"Has anyone complained about communication with the hospital staff?\"\n",
        "\n",
        "qa_chain.invoke({\"question\": question})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af1NhP-2w3SF",
        "outputId": "16a796af-d47a-47d9-e769-94bf342c5da7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': \"what's my previous question?\",\n",
              " 'chat_history': [HumanMessage(content='Has anyone complained about communication with the hospital staff?'),\n",
              "  AIMessage(content='Yes, Terri Smith has complained about the communication between the medical staff and herself at the hospital. She mentioned that the unclear communication led to misunderstandings about her treatment plan and expressed the need for improvement in this area.'),\n",
              "  HumanMessage(content=\"what's my previous question?\"),\n",
              "  AIMessage(content=\"The previous question was asking about the patient's experience at the hospital, specifically regarding the medical care received, the hospital facilities, and the communication about the daily schedule and procedures.\")],\n",
              " 'answer': \"The previous question was asking about the patient's experience at the hospital, specifically regarding the medical care received, the hospital facilities, and the communication about the daily schedule and procedures.\"}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question2 = \"what's my previous question?\"\n",
        "\n",
        "qa_chain.invoke({\"question\": question2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNUgGxlcHolh"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vXSriZvIEDC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def get_current_wait_time(hospital: str) -> int | str:\n",
        "    \"\"\"Dummy function to generate fake wait times\"\"\"\n",
        "\n",
        "    if hospital not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        return f\"Hospital {hospital} does not exist\"\n",
        "\n",
        "    # Simulate API call delay\n",
        "    time.sleep(1)\n",
        "\n",
        "    return random.randint(0, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s-iBc1pIar7"
      },
      "outputs": [],
      "source": [
        "# !pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruWfvks4HoKm"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.agents import (\n",
        "    create_openai_functions_agent,\n",
        "    Tool,\n",
        "    AgentExecutor,\n",
        ")\n",
        "from langchain import hub\n",
        "\n",
        "# ...\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Reviews\",\n",
        "        func=review_chain.invoke,\n",
        "        description=\"\"\"Useful when you need to answer questions\n",
        "        about patient reviews or experiences at the hospital.\n",
        "        Not useful for answering questions about specific visit\n",
        "        details such as payer, billing, treatment, diagnosis,\n",
        "        chief complaint, hospital, or physician information.\n",
        "        Pass the entire question as input to the tool. For instance,\n",
        "        if the question is \"What do patients think about the triage system?\",\n",
        "        the input should be \"What do patients think about the triage system?\"\n",
        "        \"\"\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Waits\",\n",
        "        func=get_current_wait_time,\n",
        "        description=\"\"\"Use when asked about current wait times\n",
        "        at a specific hospital. This tool can only get the current\n",
        "        wait time at a hospital and does not have any information about\n",
        "        aggregate or historical wait times. This tool returns wait times in\n",
        "        minutes. Do not pass the word \"hospital\" as input,\n",
        "        only the hospital name itself. For instance, if the question is\n",
        "        \"What is the wait time at hospital A?\", the input should be \"A\".\n",
        "        \"\"\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "hospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "agent_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    temperature=0,\n",
        "    openai_api_key = OPEN_API_KEY,\n",
        ")\n",
        "\n",
        "hospital_agent = create_openai_functions_agent(\n",
        "    llm=agent_chat_model,\n",
        "    prompt=hospital_agent_prompt,\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "hospital_agent_executor = AgentExecutor(\n",
        "    agent=hospital_agent,\n",
        "    tools=tools,\n",
        "    return_intermediate_steps=True,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJG-9FLaIm3_",
        "outputId": "35f8f242-ec36-483a-f333-f9ac00affb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Waits` with `C`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m4359\u001b[0m\u001b[32;1m\u001b[1;3mThe current wait time at hospital C is 43 minutes and 59 seconds.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Reviews` with `What have patients said about their comfort at the hospital?`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPatients have mentioned that the hospital's dedication to patient comfort was evident in well-designed private rooms and comfortable furnishings. This made their recovery more bearable and contributed to an overall positive experience. However, one patient did mention that the uncomfortable beds made it difficult to get a good night's sleep during their stay.\u001b[0m\u001b[32;1m\u001b[1;3mPatients have mentioned that the hospital's dedication to patient comfort was evident in well-designed private rooms and comfortable furnishings. This made their recovery more bearable and contributed to an overall positive experience. However, one patient did mention that the uncomfortable beds made it difficult to get a good night's sleep during their stay.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What have patients said about their comfort at the hospital?',\n",
              " 'output': \"Patients have mentioned that the hospital's dedication to patient comfort was evident in well-designed private rooms and comfortable furnishings. This made their recovery more bearable and contributed to an overall positive experience. However, one patient did mention that the uncomfortable beds made it difficult to get a good night's sleep during their stay.\",\n",
              " 'intermediate_steps': [(AgentActionMessageLog(tool='Reviews', tool_input='What have patients said about their comfort at the hospital?', log='\\nInvoking: `Reviews` with `What have patients said about their comfort at the hospital?`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"What have patients said about their comfort at the hospital?\"}', 'name': 'Reviews'}})]),\n",
              "   \"Patients have mentioned that the hospital's dedication to patient comfort was evident in well-designed private rooms and comfortable furnishings. This made their recovery more bearable and contributed to an overall positive experience. However, one patient did mention that the uncomfortable beds made it difficult to get a good night's sleep during their stay.\")]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hospital_agent_executor.invoke(\n",
        "    {\"input\": \"What is the current wait time at hospital C?\"}\n",
        ")\n",
        "\n",
        "hospital_agent_executor.invoke(\n",
        "    {\"input\": \"What have patients said about their comfort at the hospital?\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GVg2gt0H0RR"
      },
      "source": [
        "## PDF Chatbot Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRNpDWOr_F92"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO6cqesX6bul"
      },
      "outputs": [],
      "source": [
        "llm_name = \"gpt-3.5-turbo-0125\"\n",
        "def load_db(file, chain_type, k):\n",
        "    # load documents\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    # split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    # define embedding\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key = OPEN_API_KEY)\n",
        "    # create vector database from data\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    # define retriever\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    # create a chatbot chain. Memory is managed externally.\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key = OPEN_API_KEY,),\n",
        "        chain_type=chain_type,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        return_generated_question=True,\n",
        "    )\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOGN4oKM8LNT"
      },
      "outputs": [],
      "source": [
        "import panel as pn\n",
        "import param\n",
        "\n",
        "class cbfs(param.Parameterized):\n",
        "    chat_history = param.List([])\n",
        "    answer = param.String(\"\")\n",
        "    db_query  = param.String(\"\")\n",
        "    db_response = param.List([])\n",
        "\n",
        "    def __init__(self,  **params):\n",
        "        super(cbfs, self).__init__( **params)\n",
        "        self.panels = []\n",
        "        self.loaded_file = \"sample_data/MachineLearning-Lecture01.pdf\"\n",
        "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
        "\n",
        "    def call_load_db(self, count):\n",
        "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
        "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "        else:\n",
        "            file_input.save(\"temp.pdf\")  # local copy\n",
        "            self.loaded_file = file_input.filename\n",
        "            button_load.button_style=\"outline\"\n",
        "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
        "            button_load.button_style=\"solid\"\n",
        "        self.clr_history()\n",
        "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "\n",
        "    def convchain(self, query):\n",
        "        if not query:\n",
        "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
        "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
        "        self.chat_history.extend([(query, result[\"answer\"])])\n",
        "        self.db_query = result[\"generated_question\"]\n",
        "        self.db_response = result[\"source_documents\"]\n",
        "        self.answer = result['answer']\n",
        "        self.panels.extend([\n",
        "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
        "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, styles={'background-color': '#F6F6F6'}))\n",
        "        ])\n",
        "        inp.value = ''  #clears loading indicator when cleared\n",
        "        return pn.WidgetBox(*self.panels,scroll=True)\n",
        "\n",
        "    @param.depends('db_query ', )\n",
        "    def get_lquest(self):\n",
        "        if not self.db_query :\n",
        "            return pn.Column(\n",
        "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
        "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
        "            )\n",
        "        return pn.Column(\n",
        "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
        "            pn.pane.Str(self.db_query )\n",
        "        )\n",
        "\n",
        "    @param.depends('db_response', )\n",
        "    def get_sources(self):\n",
        "        if not self.db_response:\n",
        "            return\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for doc in self.db_response:\n",
        "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    @param.depends('convchain', 'clr_history')\n",
        "    def get_chats(self):\n",
        "        if not self.chat_history:\n",
        "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for exchange in self.chat_history:\n",
        "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    def clr_history(self,count=0):\n",
        "        self.chat_history = []\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIjnX5Ob1T5-"
      },
      "outputs": [],
      "source": [
        "# !pip install pypdf\n",
        "# !pip install -U docarray\n",
        "# !pip install jupyter_bokeh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cqWpHi5b8aj6",
        "outputId": "1b576243-c06b-40e4-bbca-e29d3bf60120"
      },
      "outputs": [],
      "source": [
        "cb = cbfs()\n",
        "\n",
        "file_input = pn.widgets.FileInput(accept='.pdf')\n",
        "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
        "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
        "button_clearhistory.on_click(cb.clr_history)\n",
        "inp = pn.widgets.TextInput( placeholder='Enter text hereâ€¦')\n",
        "\n",
        "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
        "conversation = pn.bind(cb.convchain, inp)\n",
        "\n",
        "# jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
        "\n",
        "tab1 = pn.Column(\n",
        "    pn.Row(inp),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab2= pn.Column(\n",
        "    pn.panel(cb.get_lquest),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(cb.get_sources ),\n",
        ")\n",
        "tab3= pn.Column(\n",
        "    pn.panel(cb.get_chats),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab4=pn.Column(\n",
        "    pn.Row( file_input, button_load, bound_button_load),\n",
        "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
        "    pn.layout.Divider(),\n",
        "    # pn.Row(jpg_pane.clone(width=400))\n",
        ")\n",
        "dashboard = pn.Column(\n",
        "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
        "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
        ")\n",
        "pn.extension()\n",
        "dashboard"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
